{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "# models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vizualization\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Check relations b/w:\n",
    "* keyword vs target\n",
    "* location vs target\n",
    "* target 1 & 0 ratio. (this ratio can be compared for training and test datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of real disaster tweets: 42.97%\n",
      "Unique keywords: 222 and unique locations: 3342\n",
      "Missing keywords in train set: 0.8%\n",
      "Missing location in train set: 33.27%\n",
      "Missing keywords in test set: 0.34%\n",
      "Missing location in test set: 14.51%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x7200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_og = pd.read_csv('./train.csv')\n",
    "test_data_og = pd.read_csv('./test.csv')\n",
    "\n",
    "# print(train_data_og.dtypes)\n",
    "# display(train_data_og.sample(n= 5).style)\n",
    "# print(train_data_og.isnull().sum(), test_data_og.isnull().sum())\n",
    "\n",
    "train_data = train_data_og.copy()\n",
    "print(f\"% of real disaster tweets: {np.round(np.sum(train_data['target'])/len(train_data)*100,2)}%\")\n",
    "train_data['target_sum'] = train_data.groupby('keyword')['target'].transform('sum')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 72), dpi=100)\n",
    "# sns.countplot(data = train_data, y = train_data.sort_values(by='target_sum', ascending= False)['keyword'],  hue='target')\n",
    "train_data.drop(columns=['target_sum'], inplace=True)\n",
    "\n",
    "train_data['target_sum'] = train_data.groupby('location')['target'].transform('sum')\n",
    "# sns.countplot(data = train_data, y = train_data.sort_values(by='target_sum', ascending= False)['location'].head(800),  hue='target')\n",
    "train_data.drop(columns=['target_sum'], inplace=True)\n",
    "\n",
    "print(f\"Unique keywords: {len(train_data['keyword'].unique())} and unique locations: {len(train_data['location'].unique())}\")\n",
    "print(f\"Missing keywords in train set: {np.round(train_data['keyword'].isnull().sum()/len(train_data)*100,2)}%\")\n",
    "print(f\"Missing location in train set: {np.round(train_data['location'].isnull().sum()/len(train_data)*100,2)}%\")\n",
    "print(f\"Missing keywords in test set: {np.round(test_data_og['keyword'].isnull().sum()/len(train_data)*100,2)}%\")\n",
    "print(f\"Missing location in test set: {np.round(test_data_og['location'].isnull().sum()/len(train_data)*100,2)}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "* We have almost a balanced set for classification. 42.97% are real and rest are fake disasters.\n",
    "* Unique keywords: 222 and unique locations: 3342 (including null)\n",
    "* Clearly, many keywords have high/low ratio of target count. ie, they can be used to identify target.\n",
    "* % of missing locations is high in train set (>30%) and test set (>10%).\n",
    "* locations are not good indicator of a real/fake disaster. Only a few of them show a correlation. No need to use this as a feature, for now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "* Deal with missing keywords\n",
    "* (DONE) perform word tokenization\n",
    "* (DONE) Remove stop words\n",
    "* (DONE) Remove https links: checking their relavence is out of scope.\n",
    "* (DONE) Remove punctuations\n",
    "* (DONE) lowercase strings\n",
    "* (DONE) apply stemming : reducing words to their stem/root words by removing suffixes\n",
    "* (X) apply lemmatization: reducing words to its lexeme form or inflected form. words used in the same context.\n",
    "* vectorization: Bag of Words\n",
    "\n",
    "Lemmatization is more complex than stemming:\n",
    "* it needs parts of speech, if its done for individual words otherwise there is no way to understand the context of the word.\n",
    "* to get this POS we need a lookup dictionary like WordNet (by princeton) and then convert this tag to a tag that nltk will understand.\n",
    "* no need to do stemming and lemmatization together. choose the one which is good enough. Ofc, lemmatization is more like fine tuning.\n",
    "* Apply lemmatization before removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c4273\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c4273_level0_col0\" class=\"col_heading level0 col0\" >id</th>\n",
       "      <th id=\"T_c4273_level0_col1\" class=\"col_heading level0 col1\" >keyword</th>\n",
       "      <th id=\"T_c4273_level0_col2\" class=\"col_heading level0 col2\" >location</th>\n",
       "      <th id=\"T_c4273_level0_col3\" class=\"col_heading level0 col3\" >text</th>\n",
       "      <th id=\"T_c4273_level0_col4\" class=\"col_heading level0 col4\" >target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c4273_level0_row0\" class=\"row_heading level0 row0\" >707</th>\n",
       "      <td id=\"T_c4273_row0_col0\" class=\"data row0 col0\" >1021</td>\n",
       "      <td id=\"T_c4273_row0_col1\" class=\"data row0 col1\" >blazing</td>\n",
       "      <td id=\"T_c4273_row0_col2\" class=\"data row0 col2\" >New York</td>\n",
       "      <td id=\"T_c4273_row0_col3\" class=\"data row0 col3\" >['morgan', 'silver', 'dollar', 'gem', 'blaze', 'satin', 'rare', 'proof', 'like', 'full']</td>\n",
       "      <td id=\"T_c4273_row0_col4\" class=\"data row0 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4273_level0_row1\" class=\"row_heading level0 row1\" >6043</th>\n",
       "      <td id=\"T_c4273_row1_col0\" class=\"data row1 col0\" >8638</td>\n",
       "      <td id=\"T_c4273_row1_col1\" class=\"data row1 col1\" >seismic</td>\n",
       "      <td id=\"T_c4273_row1_col2\" class=\"data row1 col2\" >nan</td>\n",
       "      <td id=\"T_c4273_row1_col3\" class=\"data row1 col3\" >['subcontractor', 'work', 'french', 'seismic', 'survey', 'group', 'cgg', 'kidnap', 'cairo', 'held', 'islam', 'state', 'compani', 'said']</td>\n",
       "      <td id=\"T_c4273_row1_col4\" class=\"data row1 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4273_level0_row2\" class=\"row_heading level0 row2\" >2931</th>\n",
       "      <td id=\"T_c4273_row2_col0\" class=\"data row2 col0\" >4212</td>\n",
       "      <td id=\"T_c4273_row2_col1\" class=\"data row2 col1\" >drowned</td>\n",
       "      <td id=\"T_c4273_row2_col2\" class=\"data row2 col2\" >Dreieich, Germany</td>\n",
       "      <td id=\"T_c4273_row2_col3\" class=\"data row2 col3\" >['via', 'dwenglish', 'hundr', 'fear', 'drown', 'migrant', 'boat', 'capsiz', 'libya', 'ufoublogeurop']</td>\n",
       "      <td id=\"T_c4273_row2_col4\" class=\"data row2 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4273_level0_row3\" class=\"row_heading level0 row3\" >4418</th>\n",
       "      <td id=\"T_c4273_row3_col0\" class=\"data row3 col0\" >6281</td>\n",
       "      <td id=\"T_c4273_row3_col1\" class=\"data row3 col1\" >hijacking</td>\n",
       "      <td id=\"T_c4273_row3_col2\" class=\"data row3 col2\" >nan</td>\n",
       "      <td id=\"T_c4273_row3_col3\" class=\"data row3 col3\" >['murder', 'stori', 'america', 'first', 'hijack']</td>\n",
       "      <td id=\"T_c4273_row3_col4\" class=\"data row3 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c4273_level0_row4\" class=\"row_heading level0 row4\" >2338</th>\n",
       "      <td id=\"T_c4273_row4_col0\" class=\"data row4 col0\" >3364</td>\n",
       "      <td id=\"T_c4273_row4_col1\" class=\"data row4 col1\" >demolition</td>\n",
       "      <td id=\"T_c4273_row4_col2\" class=\"data row4 col2\" >Murray Hill, New Jersey</td>\n",
       "      <td id=\"T_c4273_row4_col3\" class=\"data row4 col3\" >['remain', 'section', 'greyston', 'psychiatr', 'hospit', 'demolit', 'pic']</td>\n",
       "      <td id=\"T_c4273_row4_col4\" class=\"data row4 col4\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x190c85b2490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove urls, numbers, eveything except strings, alphanum,hashtags and spaces.\n",
    "train_data= train_data.replace(to_replace= {'text':{r'http\\S+':'',r'[0-9]+':'',r'[^A-Za-z0-9# ]+':''}}, regex=True)\n",
    "\n",
    "train_data['text'] = train_data['text'].str.lower().apply(word_tokenize)\n",
    "\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "punctuation_en = set(punctuation)\n",
    "stopwords_punctuations_en = stopwords_en.union(punctuation_en)\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(lambda x: [word for word in x if word not in stopwords_punctuations_en and len(word)>2 ])\n",
    "\n",
    "porter = PorterStemmer()\n",
    "train_data['text'] = train_data['text'].apply(lambda x : [porter.stem(word) for word in x ] )\n",
    "\n",
    "display(train_data.sample(n=5).style)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "* (DONE) Some tokens contain: \"'s\", \"--\", decimal numbers etc.\n",
    "* (DONE) Tweets containing consecutive # hashtags are concatenated together because we remove special chars first then tokenize. This was many tokens will be unused for classification. Check train_data.loc[6626,'text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "* Bag of Words\n",
    "* TFIDF\n",
    "* Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[243], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m bow \u001b[39m=\u001b[39m CountVectorizer(lowercase\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, analyzer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mword\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m x \u001b[39m=\u001b[39m bow\u001b[39m.\u001b[39;49mfit_transform(train_data[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(x)\n",
      "File \u001b[1;32me:\\Coding prac\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1377\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1369\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1370\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1374\u001b[0m             )\n\u001b[0;32m   1375\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1380\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32me:\\Coding prac\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1264\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1263\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1264\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1265\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1266\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32me:\\Coding prac\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m     doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     doc \u001b[39m=\u001b[39m tokenizer(doc)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "bow = CountVectorizer(preprocessor=preprocess_text)\n",
    "x = bow.fit_transform(train_data['text'])\n",
    "\n",
    "print(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coding prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f58c01d3280ef83fc61600421c7c5ac6e7ea2fe2e0e9fc76fa916fea3bc77b6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
