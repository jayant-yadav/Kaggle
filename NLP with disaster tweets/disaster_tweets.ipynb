{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer \n",
    "from io import StringIO\n",
    "\n",
    "# models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vizualization\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Check relations b/w:\n",
    "* keyword vs target\n",
    "* location vs target\n",
    "* target 1 & 0 ratio. (this ratio can be compared for training and test datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of real disaster tweets: 42.97%\n",
      "Unique keywords: 222 and unique locations: 3342\n",
      "Missing keywords in train set: 0.8%\n",
      "Missing location in train set: 33.27%\n",
      "Missing keywords in test set: 0.34%\n",
      "Missing location in test set: 14.51%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x7200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_og = pd.read_csv('./train.csv')\n",
    "test_data_og = pd.read_csv('./test.csv')\n",
    "\n",
    "# print(train_data_og.dtypes)\n",
    "# display(train_data_og.sample(n= 5).style)\n",
    "# print(train_data_og.isnull().sum(), test_data_og.isnull().sum())\n",
    "\n",
    "train_data = train_data_og.copy()\n",
    "print(f\"% of real disaster tweets: {np.round(np.sum(train_data['target'])/len(train_data)*100,2)}%\")\n",
    "train_data['target_sum'] = train_data.groupby('keyword')['target'].transform('sum')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 72), dpi=100)\n",
    "# sns.countplot(data = train_data, y = train_data.sort_values(by='target_sum', ascending= False)['keyword'],  hue='target')\n",
    "train_data.drop(columns=['target_sum'], inplace=True)\n",
    "\n",
    "train_data['target_sum'] = train_data.groupby('location')['target'].transform('sum')\n",
    "# sns.countplot(data = train_data, y = train_data.sort_values(by='target_sum', ascending= False)['location'].head(800),  hue='target')\n",
    "train_data.drop(columns=['target_sum'], inplace=True)\n",
    "\n",
    "print(f\"Unique keywords: {len(train_data['keyword'].unique())} and unique locations: {len(train_data['location'].unique())}\")\n",
    "print(f\"Missing keywords in train set: {np.round(train_data['keyword'].isnull().sum()/len(train_data)*100,2)}%\")\n",
    "print(f\"Missing location in train set: {np.round(train_data['location'].isnull().sum()/len(train_data)*100,2)}%\")\n",
    "print(f\"Missing keywords in test set: {np.round(test_data_og['keyword'].isnull().sum()/len(train_data)*100,2)}%\")\n",
    "print(f\"Missing location in test set: {np.round(test_data_og['location'].isnull().sum()/len(train_data)*100,2)}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "* We have almost a balanced set for classification. 42.97% are real and rest are fake disasters.\n",
    "* Unique keywords: 222 and unique locations: 3342 (including null)\n",
    "* Clearly, many keywords have high/low ratio of target count. ie, they can be used to identify target.\n",
    "* % of missing locations is high in train set (>30%) and test set (>10%).\n",
    "* locations are not good indicator of a real/fake disaster. Only a few of them show a correlation. No need to use this as a feature, for now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "* Deal with missing keywords\n",
    "* (DONE) perform word tokenization\n",
    "* (DONE) Remove stop words\n",
    "* (DONE) Remove https links: checking their relavence is out of scope.\n",
    "* (DONE) Remove punctuations\n",
    "* (DONE) lowercase strings\n",
    "* (DONE) apply stemming : reducing words to their stem/root words by removing suffixes\n",
    "* (X) apply lemmatization: reducing words to its lexeme form or inflected form. words used in the same context.\n",
    "\n",
    "\n",
    "Lemmatization is more complex than stemming:\n",
    "* it needs parts of speech, if its done for individual words otherwise there is no way to understand the context of the word.\n",
    "* to get this POS we need a lookup dictionary like WordNet (by princeton) and then convert this tag to a tag that nltk will understand.\n",
    "* no need to do stemming and lemmatization together. choose the one which is good enough. Ofc, lemmatization is more like fine tuning.\n",
    "* Apply lemmatization before removing stop words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_4b172\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4b172_level0_col0\" class=\"col_heading level0 col0\" >id</th>\n",
       "      <th id=\"T_4b172_level0_col1\" class=\"col_heading level0 col1\" >keyword</th>\n",
       "      <th id=\"T_4b172_level0_col2\" class=\"col_heading level0 col2\" >location</th>\n",
       "      <th id=\"T_4b172_level0_col3\" class=\"col_heading level0 col3\" >text</th>\n",
       "      <th id=\"T_4b172_level0_col4\" class=\"col_heading level0 col4\" >target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4b172_level0_row0\" class=\"row_heading level0 row0\" >3998</th>\n",
       "      <td id=\"T_4b172_row0_col0\" class=\"data row0 col0\" >5677</td>\n",
       "      <td id=\"T_4b172_row0_col1\" class=\"data row0 col1\" >floods</td>\n",
       "      <td id=\"T_4b172_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
       "      <td id=\"T_4b172_row0_col3\" class=\"data row0 col3\" >dead due flood myanmar naypyidaw aug prensa latina death toll rose today myanmar</td>\n",
       "      <td id=\"T_4b172_row0_col4\" class=\"data row0 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4b172_level0_row1\" class=\"row_heading level0 row1\" >6589</th>\n",
       "      <td id=\"T_4b172_row1_col0\" class=\"data row1 col0\" >9435</td>\n",
       "      <td id=\"T_4b172_row1_col1\" class=\"data row1 col1\" >survivors</td>\n",
       "      <td id=\"T_4b172_row1_col2\" class=\"data row1 col2\" >Anywhere Safe</td>\n",
       "      <td id=\"T_4b172_row1_col3\" class=\"data row1 col3\" >lawfulsurvivor tdog hole apart store sever survivor glenn moral andrea jacqui merl</td>\n",
       "      <td id=\"T_4b172_row1_col4\" class=\"data row1 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4b172_level0_row2\" class=\"row_heading level0 row2\" >172</th>\n",
       "      <td id=\"T_4b172_row2_col0\" class=\"data row2 col0\" >247</td>\n",
       "      <td id=\"T_4b172_row2_col1\" class=\"data row2 col1\" >ambulance</td>\n",
       "      <td id=\"T_4b172_row2_col2\" class=\"data row2 col2\" >Jackson</td>\n",
       "      <td id=\"T_4b172_row2_col3\" class=\"data row2 col3\" >twelv fear kill pakistani air ambul helicopt crash</td>\n",
       "      <td id=\"T_4b172_row2_col4\" class=\"data row2 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4b172_level0_row3\" class=\"row_heading level0 row3\" >7178</th>\n",
       "      <td id=\"T_4b172_row3_col0\" class=\"data row3 col0\" >10287</td>\n",
       "      <td id=\"T_4b172_row3_col1\" class=\"data row3 col1\" >weapon</td>\n",
       "      <td id=\"T_4b172_row3_col2\" class=\"data row3 col2\" >//RP\\ ot @Mort3mer\\\\</td>\n",
       "      <td id=\"T_4b172_row3_col3\" class=\"data row3 col3\" >honey aint angel like scream word weapon well ahead take best shot woman wan leav</td>\n",
       "      <td id=\"T_4b172_row3_col4\" class=\"data row3 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4b172_level0_row4\" class=\"row_heading level0 row4\" >1928</th>\n",
       "      <td id=\"T_4b172_row4_col0\" class=\"data row4 col0\" >2771</td>\n",
       "      <td id=\"T_4b172_row4_col1\" class=\"data row4 col1\" >curfew</td>\n",
       "      <td id=\"T_4b172_row4_col2\" class=\"data row4 col2\" >nan</td>\n",
       "      <td id=\"T_4b172_row4_col3\" class=\"data row4 col3\" >aptlyengineerd curfew</td>\n",
       "      <td id=\"T_4b172_row4_col4\" class=\"data row4 col4\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1908c418df0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove urls, numbers, eveything except strings, alphanum,hashtags and spaces.\n",
    "train_data= train_data.replace(to_replace= {'text':{r'http\\S+':'',r'[0-9]+':'',r'[^A-Za-z0-9# ]+':''}}, regex=True)\n",
    "\n",
    "train_data['text'] = train_data['text'].str.lower().apply(word_tokenize)\n",
    "\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "punctuation_en = set(punctuation)\n",
    "stopwords_punctuations_en = stopwords_en.union(punctuation_en)\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(lambda x: [word for word in x if word not in stopwords_punctuations_en and len(word)>2 ])\n",
    "\n",
    "porter = PorterStemmer()\n",
    "train_data['text'] = train_data['text'].apply(lambda x : ' '.join([porter.stem(word) for word in x ]) ) \n",
    "#converting back to string because CountVectorizer inputs Sting and not list\n",
    "\n",
    "display(train_data.sample(n=5).style)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "* (DONE) Some tokens contain: \"'s\", \"--\", decimal numbers etc.\n",
    "* (DONE) Tweets containing consecutive # hashtags are concatenated together because we remove special chars first then tokenize. This was many tokens will be unused for classification. Check train_data.loc[6626,'text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "* Bag of Words\n",
    "* TFIDF  \n",
    "We use the preprocessing steps tested out above to create a func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_a1af9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a1af9_level0_col0\" class=\"col_heading level0 col0\" >id</th>\n",
       "      <th id=\"T_a1af9_level0_col1\" class=\"col_heading level0 col1\" >keyword</th>\n",
       "      <th id=\"T_a1af9_level0_col2\" class=\"col_heading level0 col2\" >location</th>\n",
       "      <th id=\"T_a1af9_level0_col3\" class=\"col_heading level0 col3\" >text</th>\n",
       "      <th id=\"T_a1af9_level0_col4\" class=\"col_heading level0 col4\" >target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a1af9_level0_row0\" class=\"row_heading level0 row0\" >33</th>\n",
       "      <td id=\"T_a1af9_row0_col0\" class=\"data row0 col0\" >50</td>\n",
       "      <td id=\"T_a1af9_row0_col1\" class=\"data row0 col1\" >ablaze</td>\n",
       "      <td id=\"T_a1af9_row0_col2\" class=\"data row0 col2\" >AFRICA</td>\n",
       "      <td id=\"T_a1af9_row0_col3\" class=\"data row0 col3\" >africanbaz break newsnigeria flag set ablaz aba</td>\n",
       "      <td id=\"T_a1af9_row0_col4\" class=\"data row0 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a1af9_level0_row1\" class=\"row_heading level0 row1\" >2766</th>\n",
       "      <td id=\"T_a1af9_row1_col0\" class=\"data row1 col0\" >3975</td>\n",
       "      <td id=\"T_a1af9_row1_col1\" class=\"data row1 col1\" >devastation</td>\n",
       "      <td id=\"T_a1af9_row1_col2\" class=\"data row1 col2\" >Devon/London </td>\n",
       "      <td id=\"T_a1af9_row1_col3\" class=\"data row1 col3\" >devast smash phone</td>\n",
       "      <td id=\"T_a1af9_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a1af9_level0_row2\" class=\"row_heading level0 row2\" >804</th>\n",
       "      <td id=\"T_a1af9_row2_col0\" class=\"data row2 col0\" >1167</td>\n",
       "      <td id=\"T_a1af9_row2_col1\" class=\"data row2 col1\" >blight</td>\n",
       "      <td id=\"T_a1af9_row2_col2\" class=\"data row2 col2\" >Vancouver, BC</td>\n",
       "      <td id=\"T_a1af9_row2_col3\" class=\"data row2 col3\" >parksboardfact first zippolin one want use commun never ask blight park moveit</td>\n",
       "      <td id=\"T_a1af9_row2_col4\" class=\"data row2 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a1af9_level0_row3\" class=\"row_heading level0 row3\" >527</th>\n",
       "      <td id=\"T_a1af9_row3_col0\" class=\"data row3 col0\" >762</td>\n",
       "      <td id=\"T_a1af9_row3_col1\" class=\"data row3 col1\" >avalanche</td>\n",
       "      <td id=\"T_a1af9_row3_col2\" class=\"data row3 col2\" >Score Team Goals Buying @</td>\n",
       "      <td id=\"T_a1af9_row3_col3\" class=\"data row3 col3\" >tix calgari flame col avalanch preseason scotiabank saddledom</td>\n",
       "      <td id=\"T_a1af9_row3_col4\" class=\"data row3 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a1af9_level0_row4\" class=\"row_heading level0 row4\" >371</th>\n",
       "      <td id=\"T_a1af9_row4_col0\" class=\"data row4 col0\" >531</td>\n",
       "      <td id=\"T_a1af9_row4_col1\" class=\"data row4 col1\" >army</td>\n",
       "      <td id=\"T_a1af9_row4_col2\" class=\"data row4 col2\" >nan</td>\n",
       "      <td id=\"T_a1af9_row4_col3\" class=\"data row4 col3\" >beyonc pick fan armi beyhiv</td>\n",
       "      <td id=\"T_a1af9_row4_col4\" class=\"data row4 col4\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19085273460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "  (0, 3029)\t1\n",
      "  (0, 9763)\t1\n",
      "  (0, 3614)\t1\n",
      "  (0, 7434)\t1\n",
      "  (0, 321)\t1\n",
      "  (0, 4451)\t1\n",
      "  (1, 4443)\t1\n",
      "  (1, 4293)\t1\n",
      "  (1, 8144)\t1\n",
      "  (1, 10181)\t1\n",
      "  (1, 10423)\t1\n",
      "  (1, 1805)\t1\n",
      "  (2, 9954)\t1\n",
      "  (2, 697)\t1\n",
      "  (2, 10738)\t2\n",
      "  (2, 9149)\t2\n",
      "  (2, 8387)\t1\n",
      "  (2, 8520)\t1\n",
      "  (2, 3947)\t1\n",
      "  (2, 8685)\t1\n",
      "  (2, 4012)\t1\n",
      "  (3, 3947)\t1\n",
      "  (3, 8685)\t1\n",
      "  (3, 8988)\t1\n",
      "  (3, 9780)\t1\n",
      "  :\t:\n",
      "  (7609, 12037)\t1\n",
      "  (7610, 12961)\t1\n",
      "  (7610, 5255)\t1\n",
      "  (7610, 12774)\t1\n",
      "  (7611, 1853)\t1\n",
      "  (7611, 9232)\t1\n",
      "  (7611, 10052)\t1\n",
      "  (7611, 5913)\t1\n",
      "  (7611, 6993)\t1\n",
      "  (7611, 10641)\t1\n",
      "  (7611, 6004)\t1\n",
      "  (7611, 12096)\t1\n",
      "  (7611, 11561)\t1\n",
      "  (7611, 2370)\t1\n",
      "  (7611, 3635)\t2\n",
      "  (7611, 9278)\t1\n",
      "  (7611, 8334)\t1\n",
      "  (7612, 13251)\t1\n",
      "  (7612, 1766)\t1\n",
      "  (7612, 5493)\t1\n",
      "  (7612, 8210)\t1\n",
      "  (7612, 6745)\t1\n",
      "  (7612, 8362)\t1\n",
      "  (7612, 21)\t1\n",
      "  (7612, 9720)\t1\n"
     ]
    }
   ],
   "source": [
    "bow = CountVectorizer(stop_words=None, analyzer='word')\n",
    "display(train_data.sample(n=5).style)\n",
    "print(train_data['text'].dtype)\n",
    "x = bow.fit_transform(train_data['text'])\n",
    "\n",
    "print(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "* Multinomial Naive Bayes: good for text classification where data is represented as word counts, ie multinomially distributed data.\n",
    "* Complement Naive Bayes: faster and better than MNB. Takes features not present in a class for all documents, to learn.\n",
    "* Ridge Regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coding prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f58c01d3280ef83fc61600421c7c5ac6e7ea2fe2e0e9fc76fa916fea3bc77b6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
