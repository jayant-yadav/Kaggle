{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer \n",
    "from io import StringIO\n",
    "\n",
    "# models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# vizualization\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Check relations b/w:\n",
    "* keyword vs target\n",
    "* location vs target\n",
    "* target 1 & 0 ratio. (this ratio can be compared for training and test datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of real disaster tweets: 42.97%\n",
      "Unique keywords: 222 and unique locations: 3342\n",
      "Missing keywords in train set: 0.8%\n",
      "Missing location in train set: 33.27%\n",
      "Missing keywords in test set: 0.34%\n",
      "Missing location in test set: 14.51%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x7200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_og = pd.read_csv('./train.csv')\n",
    "test_data_og = pd.read_csv('./test.csv')\n",
    "\n",
    "# print(train_data_og.dtypes)\n",
    "# display(train_data_og.sample(n= 5).style)\n",
    "# print(train_data_og.isnull().sum(), test_data_og.isnull().sum())\n",
    "\n",
    "train_data = train_data_og.copy()\n",
    "test_data = test_data_og.copy()\n",
    "print(f\"% of real disaster tweets: {np.round(np.sum(train_data['target'])/len(train_data)*100,2)}%\")\n",
    "train_data['target_sum'] = train_data.groupby('keyword')['target'].transform('sum')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 72), dpi=100)\n",
    "# sns.countplot(data = train_data, y = train_data.sort_values(by='target_sum', ascending= False)['keyword'],  hue='target')\n",
    "train_data.drop(columns=['target_sum'], inplace=True)\n",
    "\n",
    "train_data['target_sum'] = train_data.groupby('location')['target'].transform('sum')\n",
    "# sns.countplot(data = train_data, y = train_data.sort_values(by='target_sum', ascending= False)['location'].head(800),  hue='target')\n",
    "train_data.drop(columns=['target_sum'], inplace=True)\n",
    "\n",
    "print(f\"Unique keywords: {len(train_data['keyword'].unique())} and unique locations: {len(train_data['location'].unique())}\")\n",
    "print(f\"Missing keywords in train set: {np.round(train_data['keyword'].isnull().sum()/len(train_data)*100,2)}%\")\n",
    "print(f\"Missing location in train set: {np.round(train_data['location'].isnull().sum()/len(train_data)*100,2)}%\")\n",
    "print(f\"Missing keywords in test set: {np.round(test_data_og['keyword'].isnull().sum()/len(train_data)*100,2)}%\")\n",
    "print(f\"Missing location in test set: {np.round(test_data_og['location'].isnull().sum()/len(train_data)*100,2)}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "* We have almost a balanced set for classification. 42.97% are real and rest are fake disasters.\n",
    "* Unique keywords: 222 and unique locations: 3342 (including null)\n",
    "* Clearly, many keywords have high/low ratio of target count. ie, they can be used to identify target.\n",
    "* % of missing locations is high in train set (>30%) and test set (>10%).\n",
    "* locations are not good indicator of a real/fake disaster. Only a few of them show a correlation. No need to use this as a feature, for now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "* Deal with missing keywords\n",
    "* (DONE) perform word tokenization\n",
    "* (DONE) Remove stop words\n",
    "* (DONE) Remove https links: checking their relavence is out of scope.\n",
    "* (DONE) Remove punctuations\n",
    "* (DONE) lowercase strings\n",
    "* (DONE) apply stemming : reducing words to their stem/root words by removing suffixes\n",
    "* (X) apply lemmatization: reducing words to its lexeme form or inflected form. words used in the same context.\n",
    "\n",
    "\n",
    "Lemmatization is more complex than stemming:\n",
    "* it needs parts of speech, if its done for individual words otherwise there is no way to understand the context of the word.\n",
    "* to get this POS we need a lookup dictionary like WordNet (by princeton) and then convert this tag to a tag that nltk will understand.\n",
    "* no need to do stemming and lemmatization together. choose the one which is good enough. Ofc, lemmatization is more like fine tuning.\n",
    "* Apply lemmatization before removing stop words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls, numbers, eveything except strings, alphanum,hashtags and spaces.\n",
    "\n",
    "stopwords_json_en = set([\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"])\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "punctuation_en = set(punctuation)\n",
    "stopwords_punctuations_en = set.union(stopwords_json_en,stopwords_en,punctuation_en)\n",
    "porter = PorterStemmer()\n",
    "\n",
    "combined_dataset = [train_data,test_data]\n",
    "\n",
    "for dataset in combined_dataset:\n",
    "\n",
    "    dataset.replace(to_replace= {'text':{r'http\\S+':'',r'[0-9]+':'',r'@\\S+':'', r'[^A-Za-z0-9# ]+':''}}, regex=True, inplace=True)\n",
    "\n",
    "    dataset['text'] = dataset['text'].str.lower().apply(word_tokenize)\n",
    "\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [word for word in x if word not in stopwords_punctuations_en and len(word)>2 ])\n",
    "\n",
    "    dataset['text'] = dataset['text'].apply(lambda x : ' '.join([porter.stem(word) for word in x ]) ) \n",
    "    #converting back to string because CountVectorizer accepts Sting and not list\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "* (DONE) Some tokens contain: \"'s\", \"--\", decimal numbers etc.\n",
    "* (DONE) Tweets containing consecutive # hashtags are concatenated together because we remove special chars first then tokenize. This was many tokens will be unused for classification. Check train_data.loc[6626,'text']\n",
    "* (DONE) @someone should be removed all together and not just removing @. since this is a use handle and these words will be unnecessary in the dictionary. id: 3456\n",
    "* Use stronger/longer list of stopwords."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "* (DONE) Bag of Words\n",
    "* (DONE) TFIDF  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f2b5b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f2b5b_level0_col0\" class=\"col_heading level0 col0\" >id</th>\n",
       "      <th id=\"T_f2b5b_level0_col1\" class=\"col_heading level0 col1\" >keyword</th>\n",
       "      <th id=\"T_f2b5b_level0_col2\" class=\"col_heading level0 col2\" >location</th>\n",
       "      <th id=\"T_f2b5b_level0_col3\" class=\"col_heading level0 col3\" >text</th>\n",
       "      <th id=\"T_f2b5b_level0_col4\" class=\"col_heading level0 col4\" >target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f2b5b_level0_row0\" class=\"row_heading level0 row0\" >5300</th>\n",
       "      <td id=\"T_f2b5b_row0_col0\" class=\"data row0 col0\" >7570</td>\n",
       "      <td id=\"T_f2b5b_row0_col1\" class=\"data row0 col1\" >outbreak</td>\n",
       "      <td id=\"T_f2b5b_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
       "      <td id=\"T_f2b5b_row0_col3\" class=\"data row0 col3\" >famili sue legionnair famili affect fatal outbreak legionnair disea</td>\n",
       "      <td id=\"T_f2b5b_row0_col4\" class=\"data row0 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2b5b_level0_row1\" class=\"row_heading level0 row1\" >6490</th>\n",
       "      <td id=\"T_f2b5b_row1_col0\" class=\"data row1 col0\" >9278</td>\n",
       "      <td id=\"T_f2b5b_row1_col1\" class=\"data row1 col1\" >sunk</td>\n",
       "      <td id=\"T_f2b5b_row1_col2\" class=\"data row1 col2\" >NYC</td>\n",
       "      <td id=\"T_f2b5b_row1_col3\" class=\"data row1 col3\" >ltlt lip sunk bed arm cross head watch captain number bodi</td>\n",
       "      <td id=\"T_f2b5b_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2b5b_level0_row2\" class=\"row_heading level0 row2\" >6775</th>\n",
       "      <td id=\"T_f2b5b_row2_col0\" class=\"data row2 col0\" >9707</td>\n",
       "      <td id=\"T_f2b5b_row2_col1\" class=\"data row2 col1\" >tornado</td>\n",
       "      <td id=\"T_f2b5b_row2_col2\" class=\"data row2 col2\" >nan</td>\n",
       "      <td id=\"T_f2b5b_row2_col3\" class=\"data row2 col3\" >pretti teen hayden ryan pose strip purpl top view download video</td>\n",
       "      <td id=\"T_f2b5b_row2_col4\" class=\"data row2 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2b5b_level0_row3\" class=\"row_heading level0 row3\" >5586</th>\n",
       "      <td id=\"T_f2b5b_row3_col0\" class=\"data row3 col0\" >7971</td>\n",
       "      <td id=\"T_f2b5b_row3_col1\" class=\"data row3 col1\" >razed</td>\n",
       "      <td id=\"T_f2b5b_row3_col2\" class=\"data row3 col2\" >WorldWide</td>\n",
       "      <td id=\"T_f2b5b_row3_col3\" class=\"data row3 col3\" >news latest home raze northern california wildfir york time taf</td>\n",
       "      <td id=\"T_f2b5b_row3_col4\" class=\"data row3 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2b5b_level0_row4\" class=\"row_heading level0 row4\" >1269</th>\n",
       "      <td id=\"T_f2b5b_row4_col0\" class=\"data row4 col0\" >1831</td>\n",
       "      <td id=\"T_f2b5b_row4_col1\" class=\"data row4 col1\" >burned</td>\n",
       "      <td id=\"T_f2b5b_row4_col2\" class=\"data row4 col2\" >Erie, PA</td>\n",
       "      <td id=\"T_f2b5b_row4_col3\" class=\"data row4 col3\" >dont burn</td>\n",
       "      <td id=\"T_f2b5b_row4_col4\" class=\"data row4 col4\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19082d99730>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "11328\n"
     ]
    }
   ],
   "source": [
    "bow = TfidfVectorizer(stop_words=None, analyzer='word')\n",
    "display(train_data.sample(n=5).style)\n",
    "print(train_data['text'].dtype)\n",
    "x = bow.fit_transform(train_data['text'])\n",
    "\n",
    "print(len(bow.get_feature_names_out())) \n",
    "# before preprocessing: 21637 features\n",
    "# after preprocessing: 11328 features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "* Multinomial Naive Bayes: good for text classification where data is represented as word counts, ie multinomially distributed data.\n",
    "* Complement Naive Bayes: faster and better than MNB. Takes features not present in a class for all documents, to learn.\n",
    "* Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB classifier accuracy: 80.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(train_data['text'],train_data['target'])\n",
    "\n",
    "bow = TfidfVectorizer(stop_words=None, analyzer='word')\n",
    "train_features = bow.fit_transform(train_text)\n",
    "test_features = bow.transform(test_text)\n",
    "\n",
    "mnb = MultinomialNB(force_alpha=True)\n",
    "mnb.fit(train_features, train_labels)\n",
    "\n",
    "print(f\"Multinomial NB classifier accuracy: {np.round(mnb.score(test_features,test_labels),2)*100}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complement NB classifier accuracy: 80.0%\n"
     ]
    }
   ],
   "source": [
    "cnb = ComplementNB(force_alpha = True)\n",
    "cnb.fit(train_features,train_labels)\n",
    "\n",
    "print(f\"Complement NB classifier accuracy: {np.round(cnb.score(test_features,test_labels),2)*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge classifier accuracy: 80.0%\n"
     ]
    }
   ],
   "source": [
    "rc = RidgeClassifier()\n",
    "rc.fit(train_features, train_labels)\n",
    "\n",
    "print(f\"Ridge classifier accuracy: {np.round(rc.score(test_features,test_labels),2)*100}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(force_alpha=True)\n",
    "mnb.fit(bow.fit_transform(train_data['text']),train_data['target'])\n",
    "\n",
    "predictions = mnb.predict(bow.transform(test_data['text']))\n",
    "\n",
    "result = pd.DataFrame({'id': test_data['id'], 'target': predictions})\n",
    "\n",
    "result.to_csv('./final_submission.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score:\n",
    "* Without preprocessing: 0.79619\n",
    "* After preprocssing: 0.79221\n",
    "\n",
    "Observations:  \n",
    "* Preprocessing did not help much. maybe try different vectorization technique, NN model.\n",
    "* Using stronger stopwords brought down acc by 0.8%\n",
    "* Using TFIDF over BOW did not change the score.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coding prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f58c01d3280ef83fc61600421c7c5ac6e7ea2fe2e0e9fc76fa916fea3bc77b6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
